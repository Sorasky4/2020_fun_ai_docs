{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN AI 第2回 深層学習とPyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 深層学習とは\n",
    "\n",
    "第0回でも触れたが、深層学習の定義として、\n",
    "\n",
    "機械学習のうち\n",
    "\n",
    "- 複雑なネットワークを用いる\n",
    "\n",
    "- 人間が特徴量抽出を行わない\n",
    "\n",
    "ものと覚えておけばとりあえずいいだろう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PyTorchとは\n",
    "Pythonによる深層学習向けのフレームワーク。\n",
    "\n",
    "公式より\n",
    ">PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\n",
    "\n",
    "PytTorchとは、GPU及びCPUを使用した、深層学習のために最適化されたテンソルライブラリである。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 用語解説\n",
    "\n",
    "Python -> プログラミング言語の1つ。様々な分野のタスクを柔軟にこなせる。かつ、比較的書きやすいことから広く使われている。\n",
    "\n",
    "CPU -> Central Processing Unit の略。日本語にすると中央処理装置。PCには必ず搭載されており、キーボードやマウスなどから入力を受け取り、画面などへ出力を行う。コンピュータの制御・演算を行うことからコンピュータの頭脳と例えられることがある。\n",
    "\n",
    "GPU -> Graphics Processing Unit の略。元は描画処理特化のパーツだったが、近年GPGPU(General-purpose computing on GPU)という技術が発達し、その厖大な処理能力を、深層学習など他のタスクなどにも転用できるようになった。その結果、深層学習モデルの学習を今までよりも高速に行えるようになった。深層学習が流行した一端を担っている。\n",
    "\n",
    "テンソル -> ベクトルや行列の拡張表現。ベクトルを1階テンソル。行列を2階テンソルとし、3階、4階…と次元が上がっていく。以下にサンプルコード示し、さらに解説する。\n",
    "\n",
    "ライブラリ -> プログラミング言語におけるライブラリとは、汎用性の高いプログラムをひとまとめにしたものをいう。ライブラリ単体では動作しないことが多い。ライブラリ(図書館)から便利なプログラム(本)を引き出して使うイメージ。\n",
    "\n",
    "フレームワーク -> プログラミングにおけるフレームワークとは、それ単体でアプリケーションなどを立ち上げることができるもののことを指す。PyTorchは深層学習フレームワークであり、ライブラリである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bf{a} \\mathrm{\\,= (1, 2)} \\\\\n",
    "\\bf{b} \\mathrm{\\,= \\left(\n",
    "    \\begin{array}{c}\n",
    "    1 \\\\\n",
    "    2\n",
    "    \\end{array}\n",
    "    \\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1, 2]) #行ベクトル\n",
    "b = torch.Tensor([[1], [2]]) #列ベクトル\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bf{c} = \\left(\n",
    "    \\begin{array}{cc}\n",
    "    1 & 0\\\\\n",
    "    0 & 1\n",
    "    \\end{array}\n",
    "    \\right)\n",
    "$$    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[1., 0.],\n        [0., 1.]])\n"
    }
   ],
   "source": [
    "c = torch.Tensor([[1, 0], [0, 1]]) #2x2の行列\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([5.])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a,b) #aとbの内積"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda429541fdaed24deaa7378a94fef3496a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}