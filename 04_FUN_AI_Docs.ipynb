{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "***\n",
    "\n",
    "## 概要  \n",
    "人間の視覚の仕組みを取り入れ成功したモデルである。現在も広く使われており、その応用範囲はとても広い。画像認識、クラス分類、など画像が絡むタスクはだいたいCNNを用いて解決される. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用語説明  \n",
    "***\n",
    "### CNN\n",
    "\n",
    "畳込み層を用いるニューラルネットワークのこと。\n",
    "\n",
    "### 画像\n",
    "\n",
    "デジタル画像には、カラー画像とモノクロ(グレースケール)画像の二種類がある。カラー画像はRGBでピクセルを制御している。モノクロ画像は0~255の値で離散化して持つ\n",
    "\n",
    "### 畳込み演算\n",
    "画像など(一般にtensor)に対して、各要素ごとにフィルタを積和演算する演算のこと。以下の用語説明や、理論解説でも取り扱う。\n",
    "\n",
    "![画像]()\n",
    "\n",
    "### 畳込み層\n",
    "畳込み演算を行う層を畳込み層と呼ぶ。英語ではConvolution Layer\n",
    "\n",
    "### 局所受容野、カーネル、フィルター\n",
    "\n",
    "説明する人の出身領域で言葉が変わってくるが本質的には同じものを指す。神経科学系なら局所受容野、画像系ならフィルター、その他はカーネルと呼んでいる人が多い気がする。今後、この資料ではフィルタでと呼ぶ。\n",
    "\n",
    "### パディング\n",
    "\n",
    "畳込みを行うと、tensorの大きさが小さくなるから、大きさを調整するために0などで画像の周囲を埋める\n",
    "\n",
    "### ストライド\n",
    "\n",
    "フィルタをどれだけ動かすか。通常は1\n",
    "\n",
    "### 重み共有\n",
    "\n",
    "### プーリング\n",
    "\n",
    "図のような演算をプーリング(max pooling)という。\n",
    "\n",
    "### プーリング層\n",
    "\n",
    "プーリングを行う層をプーリング層という。この層を挟むことによって、フィルターの感度が鈍くなることになる。その結果、位置ずれに対する頑健性があがると言われている。一方で、特徴が失われるという否定的な意見もある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 理論解説\n",
    "***\n",
    "$$\n",
    "u_{ij}=\\sum_{p=0}^{H-1}\\sum_{q=0}^{H-1}\\,\\it{x}_{i+p\\, j+q}\\,h_{pq}\\hspace{2em}\\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装解説\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNの実装。MNISTの分類をするか\n",
    "import torch \n",
    "from torch import nn, optim\n",
    "from torch.utils.data import (Dataset, DataLoader, TensorDataset)\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(\"./Dataset/04/MNIST\",\n",
    "                   train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "mnist_test = MNIST(\"./Dataset/04/MNIST\",\n",
    "                  train=False, download=True,\n",
    "                  transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(mnist_train,\n",
    "                         batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(mnist_test,\n",
    "                        batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (N, C, H, W) 形式のTensorを(N, C*H*W) に引き伸ばす\n",
    "# 畳込み層の出力をMLPに渡す際に用いる\n",
    "class FlattenLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        sizes = x.size()\n",
    "        return x.view(sizes[0], -1)\n",
    "    \n",
    "conv_net = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, 5),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.Dropout2d(0.25),\n",
    "    \n",
    "    nn.Conv2d(32, 64, 5),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.Dropout2d(0.25),\n",
    "    \n",
    "    FlattenLayer()\n",
    ")\n",
    "\n",
    "test_input = torch.ones(1, 1, 28, 28)\n",
    "conv_output_size = conv_net(test_input).size()[-1]\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(conv_output_size, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(200),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(200,10)\n",
    ")\n",
    "\n",
    "net = nn.Sequential(\n",
    "    conv_net,\n",
    "    mlp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(net, data_loader, device = 'cpu'):\n",
    "    net.eval()\n",
    "    ys = []\n",
    "    ypreds = []\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.no_grad():\n",
    "            _, y_pred = torch.max(net(x), 1)\n",
    "        ys.append(y)\n",
    "        ypreds.append(y_pred)\n",
    "        \n",
    "    ys = torch.cat(ys)\n",
    "    ypreds = torch.cat(ypreds)\n",
    "    \n",
    "    acc = (ys == ypreds).float().sum() / len (ys)\n",
    "    return acc.item()\n",
    "    \n",
    "def train_net(net, train_loader, test_loader,\n",
    "             #optimizer = optim.Adam,\n",
    "             loss_func = nn.CrossEntropyLoss(),\n",
    "             n_iter = 10, device = 'cpu'):\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    for epoch in range(n_iter):\n",
    "        running_loss = 0.\n",
    "        net.train()\n",
    "        n = 0\n",
    "        n_acc = 0\n",
    "        \n",
    "        for i, (xx, yy) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            xx = xx.to(device)\n",
    "            yy = yy.to(device)\n",
    "            h = net(xx)\n",
    "            loss = loss_fn(h, yy)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            n += len(xx)\n",
    "            _, y_pred = torch.max(h, 1)\n",
    "            n_acc += (yy == y_pred).float().sum().item()\n",
    "        train_losses.append(running_loss / i)\n",
    "        \n",
    "        train_acc.append(n_acc / n)\n",
    "        \n",
    "        val_acc.append(eval_net(net, test_loader, device))\n",
    "        \n",
    "        print(epoch, train_losses[-1], train_acc[-1], val_acc[-1], flush=True)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.to('cuda:0')\n",
    "train_net(net, train_loader, test_loader, n_iter= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演習問題"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda429541fdaed24deaa7378a94fef3496a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
